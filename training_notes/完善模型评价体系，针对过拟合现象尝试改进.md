# 完善模型评价体系，针对过拟合现象尝试改进

在昨天完成了模型搭建和训练后，针对之前评测指标不完善，过拟合严重的情况做了以下的尝试。

1.引入keras.metrics库，添加了precision（准确率）、recall（召回率）、F1-score等测试指标，其中precision和recall在metrics库中自带，而f1则没有；另外尝试了keras_metrics库，但是似乎和目前环境不兼容，原因未知。所以当前F1只能基于precision和recall用公式手动计算。当然也可以用自定义Metric类的方式自己添加f1指标，但是较麻烦。但是为了实现模型训练过程中动态保存最好f1值的模型，必须克服这个问题。

2.尝试利用正则化减弱过拟合。通过在dense层中加入正则化，过拟合有所缓解，测试集f1-score从0.74左右提高至0.76，但依然严重。此外还在attention后增加了一个dropout抑制过拟合。

3.大致学习了以下hmm和crf的原理，不是很懂，但大概知道有什么用处。

4.加入正则化后测试集评测结果：

att_dim=64

这是增加dropout前

test_loss: 4.842179298400879
test_accuracy: 0.7607843279838562
test_precision: 0.8130841255187988
test_recall: 0.6823529601097107
test_f1score: 0.7420042315355286

增加dropout后

test_loss: 4.747239112854004
test_accuracy: 0.7647058963775635
test_precision: 0.8222222328186035
test_recall: 0.7254902124404907
test_f1score: 0.7708332974208774

可以看到f1score提高了将近3个点，效果还是挺明显的，已经非常接近论文的数据了